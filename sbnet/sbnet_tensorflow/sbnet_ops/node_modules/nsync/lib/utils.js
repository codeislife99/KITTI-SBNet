var MemoryStream, Stream, async, cp, extend, fetchFile, fs, getStream, isDirectory, lsr, mkdirp, noop, parseJSON, path, readJSON, readJSONSync, readStream, stringifyJSON, waterfall, workerQueue, writeJSONSync,
  __indexOf = [].indexOf || function(item) { for (var i = 0, l = this.length; i < l; i++) { if (i in this && this[i] === item) return i; } return -1; };

async = require('async');

fs = require('fs');

MemoryStream = require('memorystream');

path = require('path');

Stream = require('stream').Stream;

noop = function() {};

isDirectory = function(filename) {

  /* Test if *filename* is a directory. */
  return filename.slice(-1) === '/';
};

waterfall = function(chain, callback) {

  /* Async waterfall that can finish on the same tick. */
  var idx, next;
  idx = 0;
  next = function(error, result) {
    var args;
    if ((error != null) || (chain[idx] == null)) {
      callback(error, result);
    } else {
      args = [next];
      if (result != null) {
        args.unshift(result);
      }
      chain[idx++].apply(null, args);
    }
  };
  chain[idx++].call(null, next);
};

workerQueue = function(worker, options) {
  var add, process, queue, workers, _ref, _ref1, _ref2, _ref3;
  if (options == null) {
    options = {};
  }

  /* Async queue implementation that handles errors. Available *options* are:
        concurrency: maximum concurrent workers running
        error: called if an error occurs
        drain: called when queue drains
        flushOnError: wether to empty the queue if an error occurs
   */
  workers = 0;
  add = function(task, toFront) {
    if (toFront) {
      queue.tasks.unshift(task);
    } else {
      queue.tasks.push(task);
    }
    setImmediate(process);
  };
  process = function() {
    var next, task;
    if (workers < queue.concurrency && queue.tasks.length) {
      task = queue.tasks.shift();
      workers += 1;
      next = function(error) {
        workers -= 1;
        if (error != null) {
          queue.error(error, task);
          if (queue.flushOnError) {
            queue.tasks = [];
            return;
          }
        }
        if (queue.tasks.length + workers === 0) {
          queue.drain();
        } else {
          process();
        }
      };
      worker(task, next);
    }
  };
  queue = {
    tasks: [],
    concurrency: (_ref = options.concurrency) != null ? _ref : 1,
    error: (_ref1 = options.error) != null ? _ref1 : noop,
    drain: (_ref2 = options.drain) != null ? _ref2 : noop,
    flushOnError: (_ref3 = options.flushOnError) != null ? _ref3 : true,
    push: function(task) {
      return add(task, false);
    },
    unshift: function(task) {
      return add(task, true);
    }
  };
  return queue;
};

lsr = function(transport, dirname, concurrency, callback) {

  /* Calls back with an array representing *dirname* on *transport*,
      including subdirectories. *concurrency* is the maximum concurrent
      listDirectory calls that will be made on *transport*.
   */
  var queue, result, worker;
  result = [];
  worker = function(dir, callback) {
    transport.listDirectory(path.join(dirname, dir), function(error, files) {
      var file, filename, _i, _len;
      if (error != null) {
        return callback(error);
      }
      for (_i = 0, _len = files.length; _i < _len; _i++) {
        file = files[_i];
        filename = path.join(dir, file);
        if (isDirectory(file)) {
          queue.push(filename);
        } else {
          result.push(filename);
        }
      }
      callback();
    });
  };
  queue = workerQueue(worker, {
    concurrency: concurrency
  });
  queue.error = callback;
  queue.drain = function() {
    return callback(null, result);
  };
  queue.push('');
};

mkdirp = function(transport, dirname, cache, callback) {

  /* Make directory *dirname* on *transport*, including intermediate directories. */
  var createDirectores, findStartDir, index, isAbsolute, parts, start;
  isAbsolute = dirname[0] === '/';
  parts = dirname.split('/').filter(function(v) {
    return v !== '.' && v !== '';
  });
  index = parts.length;
  start = 0;
  if (arguments.length === 3) {
    callback = cache;
    cache = {};
  }
  findStartDir = function(callback) {

    /* Figure out which directory we need to start creating new dirs from, if any. */
    return async.until((function() {
      return start !== 0 || index === 0;
    }), function(callback) {
      var dir;
      dir = parts.slice(0, +(index - 1) + 1 || 9e9).join('/');
      if (isAbsolute) {
        dir = '/' + dir;
      }
      if (cache[dir]) {
        start = index;
        return callback();
      }
      return transport.listDirectory(dir, function(error, list) {
        var _ref;
        if (error != null) {
          index--;
        } else {
          start = index;
          if (_ref = parts[index] + '/', __indexOf.call(list, _ref) >= 0) {
            start++;
          }
        }
        return callback();
      });
    }, callback);
  };
  createDirectores = function(callback) {

    /* Create directories in *parts* starting from *start*. */
    var create, i, _i, _ref;
    create = [];
    for (i = _i = start, _ref = parts.length; start <= _ref ? _i < _ref : _i > _ref; i = start <= _ref ? ++_i : --_i) {
      create.push(parts.slice(0, +i + 1 || 9e9).join('/'));
    }
    return async.forEachSeries(create, function(dir, callback) {
      if (isAbsolute) {
        dir = '/' + dir;
      }
      cache[dir] = true;
      return transport.makeDirectory(dir, callback);
    }, callback);
  };
  return async.series([findStartDir, createDirectores], callback);
};

cp = function(source, destination, fromFile, toFile, callback) {

  /* Copy *fromFile* on *source* transport to *toFile* on *destination* transport. */
  if (source.createReadStream != null) {
    return destination.putFile(toFile, source.createReadStream(fromFile), callback);
  } else {
    return waterfall([
      function(callback) {
        return source.getFile(fromFile, callback);
      }, function(result, callback) {
        var stream;
        if (result instanceof Stream) {
          return destination.putFile(toFile, result, callback);
        } else {
          stream = new MemoryStream;
          destination.putFile(toFile, stream, callback);
          return setImmediate(function() {
            return stream.end(result);
          });
        }
      }
    ], callback);
  }
};

getStream = function(source, filename, callback) {

  /* Callback with a readable stream for *filename* on *source* transport. */
  if (source.createReadStream != null) {
    return callback(null, source.createReadStream(filename));
  } else {
    return waterfall([
      function(callback) {
        return source.getFile(filename, callback);
      }, function(result, callback) {
        var stream;
        if (result instanceof Stream) {
          return callback(null, result);
        } else {
          stream = new MemoryStream;
          callback(null, stream);
          return setImmediate(function() {
            return stream.end(result);
          });
        }
      }
    ], callback);
  }
};

fetchFile = function(source, filename, callback) {

  /* Fetch *filename* on *source* transport and read it to memory. */
  if (source.createReadStream != null) {
    return readStream(source.createReadStream(filename), callback);
  } else {
    return waterfall([
      function(callback) {
        return source.getFile(fromFile, callback);
      }, function(result, callback) {
        if (result instanceof Stream) {
          return readStream(result, callback);
        } else {
          return callback(null, result);
        }
      }
    ], callback);
  }
};

readStream = function(stream, callback) {

  /* Reads *stream* into a Buffer. */
  var parts;
  parts = [];
  stream.on('error', function(error) {
    if (typeof callback === "function") {
      callback(error);
    }
    return callback = null;
  });
  stream.on('data', function(data) {
    return parts.push(data);
  });
  stream.on('end', function() {
    if (typeof callback === "function") {
      callback(null, Buffer.concat(parts));
    }
    return callback = null;
  });
};

parseJSON = function(buffer, callback) {

  /* Async-ish version of JSON.parse. */
  var error, rv;
  try {
    rv = JSON.parse(buffer.toString());
  } catch (_error) {
    error = _error;
    error.message = "JSON parse error: " + error.message;
    return callback(error);
  }
  return callback(null, rv);
};

stringifyJSON = function(object, callback) {

  /* Async-ish version of JSON.stringify. */
  var error, json;
  try {
    json = JSON.stringify(object, null, 2);
  } catch (_error) {
    error = _error;
    return callback(error);
  }
  return callback(null, json);
};

extend = function(object, mixin) {

  /* Extend *object* with values from *mixin*. */
  var method, name;
  for (name in mixin) {
    method = mixin[name];
    object[name] = method;
  }
};

readJSON = function(filename, callback) {

  /* Read and try to parse *filename* as JSON, *callback* with parsed object or error on fault. */
  return waterfall([
    function(callback) {
      return fs.readFile(filename, callback);
    }, function(buffer, callback) {
      var error, rv;
      try {
        rv = JSON.parse(buffer.toString());
        return callback(null, rv);
      } catch (_error) {
        error = _error;
        error.filename = filename;
        error.message = "parsing " + (path.basename(filename)) + ": " + error.message;
        return callback(error);
      }
    }
  ], callback);
};

readJSONSync = function(filename) {

  /* Synchronously read and parse *filename* as json. */
  var buffer;
  buffer = fs.readFileSync(filename);
  return JSON.parse(buffer.toString());
};

writeJSONSync = function(filename, object) {

  /* Synchronously stringify *object* to json and write it to *filename*. */
  return fs.writeFileSync(filename, JSON.stringify(object, null, 2));
};


/* Exports */

module.exports = {
  cp: cp,
  extend: extend,
  fetchFile: fetchFile,
  getStream: getStream,
  lsr: lsr,
  mkdirp: mkdirp,
  parseJSON: parseJSON,
  readJSON: readJSON,
  readJSONSync: readJSONSync,
  readStream: readStream,
  stringifyJSON: stringifyJSON,
  waterfall: waterfall,
  workerQueue: workerQueue,
  writeJSONSync: writeJSONSync
};

//# sourceMappingURL=utils.map